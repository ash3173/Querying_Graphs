{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class TemporalGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "class TemporalGraph:\n",
    "    def __init__(self, files):\n",
    "        \"\"\"\n",
    "        Initialize TemporalGraph with a list of JSON files representing graphs at different timestamps.\n",
    "        \"\"\"\n",
    "        self.files = files  # List of JSON file paths\n",
    "\n",
    "    @lru_cache(maxsize=10)  # Cache the last 10 accessed timestamps\n",
    "    def load_graph_at_timestamp(self, timestamp):\n",
    "        \"\"\"\n",
    "        Load the graph for a specific timestamp from JSON and convert it to a NetworkX graph.\n",
    "        \"\"\"\n",
    "        with open(self.files[timestamp], 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return self._json_to_graph(data)\n",
    "\n",
    "    def _json_to_graph(self, data):\n",
    "        \"\"\"\n",
    "        Convert JSON data to a NetworkX graph.\n",
    "        \"\"\"\n",
    "        graph = nx.DiGraph() if data[\"directed\"] else nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        \n",
    "        for node_type, nodes in data[\"node_values\"].items():\n",
    "            for node in nodes:\n",
    "                node_id = node[-1]  # Assuming the node ID is the last element in the list\n",
    "                node_attributes = dict(zip(data[\"node_types\"][node_type], node))\n",
    "                graph.add_node(node_id, **node_attributes)  # Add the node with its attributes\n",
    "            \n",
    "                \n",
    "        # Add edges\n",
    "        all_edge_types = data[\"relationship_types\"]\n",
    "\n",
    "        for i in data[\"relationship_values\"] :\n",
    "            \n",
    "            if i[0] in all_edge_types :\n",
    "                \n",
    "                attributes = {}\n",
    "                for j in range(len(i)-2) :\n",
    "                    key = all_edge_types[i[0]][j]\n",
    "                    attributes[key] = i[j]\n",
    "\n",
    "                graph.add_edge(i[-2],i[-1],**attributes)\n",
    "            else :\n",
    "                graph.add_edge(i[0],i[1])\n",
    "    \n",
    "        \n",
    "        return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Ensure this is imported properly\n",
    "import tracemalloc\n",
    "import functools\n",
    "\n",
    "def time_and_memory(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Start tracking memory and time\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()  # Ensure time module is used correctly\n",
    "        \n",
    "        try:\n",
    "            # Call the actual function\n",
    "            result = func(*args, **kwargs)\n",
    "        finally:\n",
    "            # Calculate memory and time usage\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            # Print results\n",
    "            print(f\"Time taken by '{func.__name__}': {elapsed_time:.2f} seconds\")\n",
    "            print(f\"Memory used by '{func.__name__}': {current / 1024:.2f} KiB (Current), {peak / 1024:.2f} KiB (Peak)\")\n",
    "\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/supply_chain_export_1000\\\\timestamp_0.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_1.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_2.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_3.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_4.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_5.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_6.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_7.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_8.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_9.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_10.json',\n",
       " 'data/supply_chain_export_1000\\\\timestamp_11.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "# Natural sorting function\n",
    "def natural_sort(files):\n",
    "    # Extract numeric parts from filenames for sorting\n",
    "    return sorted(files, key=lambda x: int(re.search(r'timestamp_(\\d+)', x).group(1)))\n",
    "\n",
    "# Get files and sort\n",
    "files = glob.glob(\"data/supply_chain_export_1000/timestamp_*.json\")\n",
    "files = natural_sort(files)\n",
    "\n",
    "# Initialize TemporalGraph\n",
    "temporal_graph = TemporalGraph(files)\n",
    "temporal_graph.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Products in a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming the TemporalGraph class has a method `load_graph_at_timestamp` for loading graphs at specific timestamps.\n",
    "\n",
    "@time_and_memory\n",
    "def query_valid_parts_nx(temporal_graph, timestamp, start_date: str, end_date: str):\n",
    "    \"\"\"\n",
    "    Retrieve valid parts based on their validity period from the NetworkX graph at a specific timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        temporal_graph (TemporalGraph): The TemporalGraph object.\n",
    "        timestamp (str): The timestamp for which to retrieve the graph.\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "        list: List of node IDs representing valid parts within the specified date range.\n",
    "    \"\"\"\n",
    "    # Convert start and end dates to datetime objects\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing dates: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Load the graph at the given timestamp\n",
    "    graph = temporal_graph.load_graph_at_timestamp(timestamp)\n",
    "    \n",
    "    # List to store valid parts (node IDs)\n",
    "    valid_parts = []\n",
    "    \n",
    "    # Iterate through the nodes in the graph\n",
    "    for node, attributes in graph.nodes(data=True):\n",
    "        # Extract valid_from and valid_till, with default empty string if not present\n",
    "        valid_from_str = attributes.get('valid_from', '')\n",
    "        valid_till_str = attributes.get('valid_till', '')\n",
    "        \n",
    "        # Only process valid nodes with valid dates\n",
    "        if valid_from_str and valid_till_str:\n",
    "            try:\n",
    "                valid_from = datetime.strptime(valid_from_str, \"%Y-%m-%d\")\n",
    "                valid_till = datetime.strptime(valid_till_str, \"%Y-%m-%d\")\n",
    "                \n",
    "                # Check if the node is valid within the given date range\n",
    "                if valid_from <= end_date and valid_till >= start_date:\n",
    "                    valid_parts.append(node)\n",
    "            except ValueError:\n",
    "                # Handle any invalid date format gracefully\n",
    "                print(f\"Skipping node {node} due to invalid date format.\")\n",
    "                continue\n",
    "    \n",
    "    return valid_parts\n",
    "\n",
    "\n",
    "@time_and_memory\n",
    "def query_valid_parts_json(temporal_graph, timestamp: int, start_date: str, end_date: str):\n",
    "    \"\"\"\n",
    "    Retrieve valid parts based on their validity period from a JSON-like structure at a specific timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        temporal_graph (TemporalGraph): The TemporalGraph object.\n",
    "        timestamp (int): The timestamp (as an integer) for which to retrieve the JSON data.\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "        list: List of node IDs representing valid parts within the specified date range.\n",
    "    \"\"\"\n",
    "    # Convert start and end dates to datetime objects\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing dates: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Load the JSON data at the given timestamp\n",
    "    with open(temporal_graph.files[timestamp], 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # List to store valid parts (node IDs)\n",
    "    valid_parts = []\n",
    "\n",
    "    # Access the node values from the data\n",
    "    node_values = data.get(\"node_values\", {}).get(\"Parts\", [])\n",
    "    \n",
    "    # Iterate over nodes to check validity dates\n",
    "    for node in node_values:\n",
    "        # Extract valid_from and valid_till\n",
    "        try:\n",
    "            valid_from = datetime.strptime(node[6], \"%Y-%m-%d\")  # Assuming 'valid_from' is at index 4\n",
    "            valid_till = datetime.strptime(node[7], \"%Y-%m-%d\")  # Assuming 'valid_till' is at index 5\n",
    "        except ValueError:\n",
    "            print(f\"Skipping node due to invalid date format: {node}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract node ID (last element in node list)\n",
    "        node_id = node[-1]\n",
    "\n",
    "        # If the node is valid within the given range, add it to the list\n",
    "        if valid_from <= end_date and valid_till >= start_date:\n",
    "            valid_parts.append(node_id)\n",
    "\n",
    "    return valid_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by 'query_valid_parts_nx': 0.51 seconds\n",
      "Memory used by 'query_valid_parts_nx': 6877.46 KiB (Current), 8792.09 KiB (Peak)\n",
      "Valid Parts from NetworkX: ['P_001', 'P_038', 'P_046', 'P_092', 'P_128', 'P_130', 'P_139', 'P_151', 'P_153', 'P_162', 'P_189', 'P_195', 'P_232', 'P_233', 'P_241', 'P_260', 'P_389', 'P_436', 'P_441', 'P_469', 'P_477', 'P_486']\n"
     ]
    }
   ],
   "source": [
    "valid_parts_nx = query_valid_parts_nx(temporal_graph,1, start_date=\"2026-12-10\", end_date=\"2027-01-17\")\n",
    "print(f\"Valid Parts from NetworkX: {valid_parts_nx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by 'query_valid_parts_json': 0.23 seconds\n",
      "Memory used by 'query_valid_parts_json': 9.62 KiB (Current), 7092.63 KiB (Peak)\n",
      "Valid Parts from JSON: ['P_001', 'P_002', 'P_008', 'P_013', 'P_014', 'P_038', 'P_043', 'P_045', 'P_046', 'P_078', 'P_080', 'P_092', 'P_097', 'P_127', 'P_128', 'P_130', 'P_138', 'P_139', 'P_151', 'P_152', 'P_153', 'P_162', 'P_163', 'P_189', 'P_195', 'P_210', 'P_215', 'P_220', 'P_232', 'P_233', 'P_240', 'P_241', 'P_254', 'P_260', 'P_261', 'P_267', 'P_281', 'P_288', 'P_300', 'P_306', 'P_323', 'P_326', 'P_327', 'P_328', 'P_329', 'P_330', 'P_333', 'P_334', 'P_336', 'P_337', 'P_355', 'P_366', 'P_372', 'P_373', 'P_389', 'P_396', 'P_400', 'P_422', 'P_425', 'P_436', 'P_441', 'P_454', 'P_465', 'P_469', 'P_477', 'P_478', 'P_481', 'P_486', 'P_488']\n"
     ]
    }
   ],
   "source": [
    "# Example for querying valid parts from JSON data\n",
    "valid_parts_json = query_valid_parts_json(temporal_graph, timestamp=0, start_date=\"2026-10-17\", end_date=\"2026-12-17\")\n",
    "print(f\"Valid Parts from JSON: {valid_parts_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N most frequent subtype in parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "@time_and_memory\n",
    "def query_most_common_subtypes_json(temporal_graph, timestamp: int, n: int):\n",
    "    \"\"\"\n",
    "    Retrieve the n most common subtypes from the JSON-like structure at a specific timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        temporal_graph (TemporalGraph): The TemporalGraph object.\n",
    "        timestamp (int): The timestamp (as an integer) for which to retrieve the JSON data.\n",
    "        n (int): The number of most common subtypes to return.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples with the subtype and its occurrence count.\n",
    "    \"\"\"\n",
    "    # Load the JSON data at the given timestamp\n",
    "    with open(temporal_graph.files[timestamp], 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # List to store subtypes\n",
    "    subtypes = []\n",
    "\n",
    "    # Access the node values from the data\n",
    "    node_values = data.get(\"node_values\", {}).get(\"Parts\", [])\n",
    "    \n",
    "    # Iterate over nodes and extract the subtypes (index 3 is the position of 'subtype' in the data schema)\n",
    "    for node in node_values:\n",
    "        subtypes.append(node[3])\n",
    "\n",
    "    # Use Counter to count occurrences of each subtype\n",
    "    subtype_counts = Counter(subtypes)\n",
    "\n",
    "    # Get the n most common subtypes\n",
    "    most_common_subtypes = subtype_counts.most_common(n)\n",
    "\n",
    "    return most_common_subtypes\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "@time_and_memory\n",
    "def query_most_common_subtypes_nx(temporal_graph, timestamp: int, n: int):\n",
    "    \"\"\"\n",
    "    Retrieve the n most common subtypes from the NetworkX graph at a specific timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        temporal_graph (TemporalGraph): The TemporalGraph object.\n",
    "        timestamp (int): The timestamp (as an integer) for which to retrieve the graph.\n",
    "        n (int): The number of most common subtypes to return.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples with the subtype and its occurrence count.\n",
    "    \"\"\"\n",
    "    # Load the graph at the given timestamp\n",
    "    graph = temporal_graph.load_graph_at_timestamp(timestamp)\n",
    "\n",
    "    # List to store subtypes\n",
    "    subtypes = []\n",
    "\n",
    "    # Iterate through the nodes in the graph and extract the 'subtype' from the node attributes\n",
    "    for node, attributes in graph.nodes(data=True):\n",
    "        subtype = attributes.get('subtype', None)\n",
    "        if subtype:\n",
    "            subtypes.append(subtype)\n",
    "\n",
    "    # Use Counter to count occurrences of each subtype\n",
    "    subtype_counts = Counter(subtypes)\n",
    "\n",
    "    # Get the n most common subtypes\n",
    "    most_common_subtypes = subtype_counts.most_common(n)\n",
    "\n",
    "    return most_common_subtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by 'query_most_common_subtypes_json': 0.20 seconds\n",
      "Memory used by 'query_most_common_subtypes_json': 11.42 KiB (Current), 7091.17 KiB (Peak)\n",
      "Most Common Subtypes from JSON: [('plastic_component', 72), ('metal_rod', 65), ('electronic_component', 59), ('metal_sheet', 59), ('chemical', 45)]\n",
      "Time taken by 'query_most_common_subtypes_nx': 0.43 seconds\n",
      "Memory used by 'query_most_common_subtypes_nx': 6873.45 KiB (Current), 8787.30 KiB (Peak)\n",
      "Most Common Subtypes from NetworkX: [('plastic_component', 72), ('metal_rod', 65), ('electronic_component', 59), ('metal_sheet', 59), ('chemical', 45)]\n"
     ]
    }
   ],
   "source": [
    "# the most common subtypes for a part from JSON data\n",
    "most_common_subtypes_json = query_most_common_subtypes_json(temporal_graph, timestamp=0, n=5)\n",
    "print(f\"Most Common Subtypes from JSON: {most_common_subtypes_json}\")\n",
    "\n",
    "# Example for querying the most common subtypes from NetworkX graph\n",
    "most_common_subtypes_nx = query_most_common_subtypes_nx(temporal_graph, timestamp=0, n=5)\n",
    "print(f\"Most Common Subtypes from NetworkX: {most_common_subtypes_nx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck Analysis Based on Part Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "@time_and_memory\n",
    "def bottleneck_parts_temporal(tg, timestamp, importance_threshold, expected_life_threshold):\n",
    "    \"\"\"\n",
    "    Perform bottleneck analysis for parts based on their attributes at a specific timestamp in a temporal graph.\n",
    "\n",
    "    Parameters:\n",
    "        tg (TemporalGraph): The temporal graph object.\n",
    "        timestamp (int): The specific timestamp to analyze.\n",
    "        importance_threshold (float): The minimum importance factor to consider.\n",
    "        expected_life_threshold (float): The maximum expected life (in days) to consider.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of bottleneck parts, including part ID, importance factor, and expected life in days.\n",
    "    \"\"\"\n",
    "    # Load the graph for the specified timestamp\n",
    "    graph = tg.load_graph_at_timestamp(timestamp)\n",
    "    \n",
    "    bottlenecks = []\n",
    "    \n",
    "    # Iterate through nodes to find parts\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if data.get(\"node_type\") == \"Parts\":\n",
    "            importance = data.get(\"importance_factor\", 0)\n",
    "            valid_from = data.get(\"valid_from\", \"1970-01-01\")\n",
    "            valid_till = data.get(\"valid_till\", \"9999-12-31\")\n",
    "\n",
    "            # Parse valid_from and valid_till as datetime objects\n",
    "            try:\n",
    "                valid_from_date = datetime.strptime(valid_from, \"%Y-%m-%d\")\n",
    "                valid_till_date = datetime.strptime(valid_till, \"%Y-%m-%d\")\n",
    "                expected_life = (valid_till_date - valid_from_date).days\n",
    "            except ValueError:\n",
    "                expected_life = float('inf')  # Handle invalid dates gracefully\n",
    "\n",
    "            # Check if part qualifies as a bottleneck\n",
    "            if importance >= importance_threshold and expected_life <= expected_life_threshold:\n",
    "                bottlenecks.append((node, importance, expected_life))\n",
    "    \n",
    "    return bottlenecks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by 'bottleneck_parts_temporal': 0.53 seconds\n",
      "Memory used by 'bottleneck_parts_temporal': 6875.78 KiB (Current), 8790.42 KiB (Peak)\n"
     ]
    }
   ],
   "source": [
    "# Assuming the TemporalGraph object is initialized\n",
    "timestamp = 3\n",
    "importance_threshold = 0.9\n",
    "expected_life_threshold = 400  # Days\n",
    "\n",
    "bottlenecks = bottleneck_parts_temporal(temporal_graph, timestamp, importance_threshold, expected_life_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck Parts:\n",
      "Part ID: P_256, Importance: 0.99, Expected Life: 360 days\n",
      "Part ID: P_377, Importance: 0.91, Expected Life: 390 days\n"
     ]
    }
   ],
   "source": [
    "print(\"Bottleneck Parts:\")\n",
    "for part in bottlenecks:\n",
    "    print(f\"Part ID: {part[0]}, Importance: {part[1]:.2f}, Expected Life: {part[2]} days\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppliers to a Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_and_memory\n",
    "def query_suppliers_for_part_via_warehouse(temporal_graph,timestamp,part_id):\n",
    "\n",
    "    G = temporal_graph.load_graph_at_timestamp(timestamp)\n",
    "    warehouses_with_part = [\n",
    "        neighbor for neighbor in G.neighbors(part_id)\n",
    "        if G[part_id][neighbor].get(\"relationship_type\") == \"WarehouseToParts\"\n",
    "    ]\n",
    "\n",
    "    suppliers = set()\n",
    "    for warehouse in warehouses_with_part:\n",
    "        for supplier in G.neighbors(warehouse):\n",
    "            if G[warehouse][supplier].get(\"relationship_type\") == \"SupplierToWarehouse\":\n",
    "                suppliers.add(supplier)\n",
    "\n",
    "    return list(suppliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by 'query_suppliers_for_part_via_warehouse': 0.00 seconds\n",
      "Memory used by 'query_suppliers_for_part_via_warehouse': 0.59 KiB (Current), 0.86 KiB (Peak)\n",
      "Suppliers for part P_001: []\n"
     ]
    }
   ],
   "source": [
    "part_id = 'P_001'  # Replace with actual part ID\n",
    "result = query_suppliers_for_part_via_warehouse(temporal_graph,0, part_id)\n",
    "print(f\"Suppliers for part {part_id}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts transported to the facility over long distances with high transportation costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Distance Impact on Costs\n",
    "def distance_impact_on_costs(temporal_graph,timestamp, distance_threshold, transport_cost_threshold):\n",
    "    results = []\n",
    "    G = temporal_graph.load_graph_at_timestamp(timestamp)\n",
    "    for u, v, data in G.edges(data=True):\n",
    "\n",
    "        if data.get(\"key\") == \"PartsToFacility\":\n",
    "            distance = data.get(\"distance_from_warehouse\", 0)\n",
    "            transport_cost = data.get(\"transport_cost\", 0)\n",
    "\n",
    "            if distance < distance_threshold and transport_cost <= transport_cost_threshold:\n",
    "                results.append((u, distance, transport_cost))\n",
    "\n",
    "    return sorted(results, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_cost_impact = distance_impact_on_costs(temporal_graph,0, distance_threshold=45,transport_cost_threshold=300)\n",
    "print(\"Parts transported over large distances:\")\n",
    "for id,distance,trans in distance_cost_impact :\n",
    "    print(\"Part ID:\", id, \",Distance:\", distance, \",Transport Cost:\",trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
